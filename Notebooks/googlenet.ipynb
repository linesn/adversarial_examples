{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial attacks on GoogleNet\n",
    "The goal of this notebook is to download a pretrained GoogleNet model for classifying CIFAR-10 images, test it on our dataset, then generate adversarial examples and see if they fool the GoogleNet model. Then we'll try transfer-training the GoogleNet model with these adversarial images to see if that makes the network robust against them, and what the accuracy cost is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model and the CIFAR10 class are provided by [Huy Phan](PyTorch_CIFAR10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import imageio\n",
    "# Model path:\n",
    "PATH = '../Models/googlenet_cifar10.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchsize = 1\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../Data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../Data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Module(pl.LightningModule):\n",
    "    def __init__(self, hparams, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.mean = [0.4914, 0.4822, 0.4465]\n",
    "        self.std = [0.2023, 0.1994, 0.2010]\n",
    "        self.model = get_classifier(hparams.classifier, pretrained)\n",
    "        self.train_size = len(self.train_dataloader().dataset)\n",
    "        self.val_size = len(self.val_dataloader().dataset)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        images, labels = batch\n",
    "        predictions = self.model(images)\n",
    "        loss = self.criterion(predictions, labels)\n",
    "        accuracy = torch.sum(torch.max(predictions, 1)[1] == labels.data).float() / batch[0].size(0)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        loss, accuracy = self.forward(batch)\n",
    "        logs = {'loss/train': loss, 'accuracy/train': accuracy}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "        \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        avg_loss, accuracy = self.forward(batch)\n",
    "        loss = avg_loss * batch[0].size(0)\n",
    "        corrects = accuracy * batch[0].size(0)\n",
    "        logs = {'loss/val': loss, 'corrects': corrects}\n",
    "        return logs\n",
    "                \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss/val'] for x in outputs]).sum() / self.val_size\n",
    "        accuracy = torch.stack([x['corrects'] for x in outputs]).sum() / self.val_size\n",
    "        logs = {'loss/val': loss, 'accuracy/val': accuracy}\n",
    "        return {'val_loss': loss, 'log': logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return self.validation_step(batch, batch_nb)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        accuracy = self.validation_epoch_end(outputs)['log']['accuracy/val']\n",
    "        accuracy = round((100 * accuracy).item(), 2)\n",
    "        return {'progress_bar': {'Accuracy': accuracy}}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.learning_rate,\n",
    "                                    weight_decay=self.hparams.weight_decay, momentum=0.9, nesterov=True)\n",
    "            \n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.hparams.learning_rate, \n",
    "                                                                     steps_per_epoch=self.train_size//self.hparams.batch_size,\n",
    "                                                                     epochs=self.hparams.max_epochs),\n",
    "                     'interval': 'step', 'name': 'learning_rate'}\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                              transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(self.mean, self.std)])\n",
    "        dataset = CIFAR10(root=self.hparams.data_dir, train=True, transform=transform_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=4, shuffle=True, drop_last=True, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        transform_val = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize(self.mean, self.std)])\n",
    "        dataset = CIFAR10(root=self.hparams.data_dir, train=False, transform=transform_val)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=4, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnet = torch.load('../Models/googlenet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(network, img):\n",
    "    \"\"\"\n",
    "    Input: Image file path (str)\n",
    "    Outputs: Predicted image class, probability assigned by network to top class\n",
    "    \"\"\"\n",
    "    img_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_tensor = img_transform(img).float().unsqueeze(0).to(device)\n",
    "    outputs = network(input_tensor).squeeze()\n",
    "    class_probas = nn.Softmax()(outputs).detach().cpu().numpy()\n",
    "    idx = np.argmax(class_probas)\n",
    "    img_class = class_names[idx]\n",
    "    proba = class_probas[idx]\n",
    "    return img_class, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adversarial_image(network, img_tuple, epsilon=0.01):\n",
    "    img_file, label = img_tuple\n",
    "    img = imageio.imread(img_file)\n",
    "    img_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_tensor = img_transform(img).float().unsqueeze(0).to(device)\n",
    "    input_tensor.requires_grad = True\n",
    "    outputs = network(input_tensor)\n",
    "    # Format label.\n",
    "    class_name = labels_class[label]\n",
    "    class_idx = class_names.index(class_name)\n",
    "    label = torch.tensor(class_idx).unsqueeze(0).to(device)\n",
    "    # Get loss gradient with regard to image pixels.\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(outputs, label)\n",
    "    loss.backward()\n",
    "    img_gradient = input_tensor.grad\n",
    "    gradient_signs = torch.sign(img_gradient).cpu().numpy().squeeze()\n",
    "    # Match shape of image (channels last in this case)\n",
    "    gradient_signs = np.transpose(gradient_signs, axes=[1, 2, 0])\n",
    "    pixel_changes = (gradient_signs * 255 * epsilon).astype(np.int16)\n",
    "    changed_img = (img).astype(np.int16) + pixel_changes\n",
    "    adv_img = np.clip(changed_img, 0, 255).astype(np.uint8)\n",
    "    return adv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot understand given URI: (tensor([[[ 0.2392,  0.2471,  0.2941,  ...,  0.0745, -0.0....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-931030af4184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ae/lib/python3.8/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ae/lib/python3.8/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ae/lib/python3.8/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ae/lib/python3.8/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_r\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0muri_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot understand given URI: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0muri_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Check if this is supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot understand given URI: (tensor([[[ 0.2392,  0.2471,  0.2941,  ...,  0.0745, -0.0...."
     ]
    }
   ],
   "source": [
    "predict_image(gnet, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the same image samples as above.\n",
    "fig, ax = plt.subplots(nrows=5, ncols=10, figsize=(24, 18))\n",
    "for i in range(50):\n",
    "    row = i // 10\n",
    "    col = i - row * 10\n",
    "    img = imageio.imread(image_samples[i][0])\n",
    "    ax[row][col].imshow(img)\n",
    "    predicted_class, predicted_proba = predict_image(net, img)\n",
    "    ax[row][col].set_xlabel(f\"{predicted_class}: {predicted_proba:.6f}\")\n",
    "    ax[row][col].set_xticks([])\n",
    "    ax[row][col].set_yticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
